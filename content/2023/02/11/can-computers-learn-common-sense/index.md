---
title: "计算机能学习常识吗？"
date: 2023-02-11T19:26:35+08:00
updated: 2023-02-11T19:26:35+08:00
taxonomies:
  tags: []
extra:
  source: https://www.newyorker.com/tech/annals-of-technology/can-computers-learn-common-sense
  hostname: www.newyorker.com
  author: Matthew Hutson
  original_title: "Can Computers Learn Common Sense?"
  original_lang: en
---

A few years ago, a computer scientist named Yejin Choi gave a presentation at an artificial-intelligence conference in New Orleans. On a screen, she projected a frame from a newscast where two anchors appeared before the headline “_CHEESEBURGER STABBING_.” Choi explained that human beings find it easy to discern the outlines of the story from those two words alone. Had someone stabbed a cheeseburger? Probably not. Had a cheeseburger been used to stab a person? Also unlikely. Had a cheeseburger stabbed a cheeseburger? Impossible.  

几年前，计算机科学家蔡业进（Yejin Choi）在新奥尔良的一次人工智能会议上做了一个报告。在屏幕上，她投射出一个新闻广播的画面，两名主播出现在标题"奶酪汉堡刺伤"之前崔解释说，人类很容易从这两个词中辨别出故事的轮廓。有人捅了芝士汉堡吗？也许不会。奶酪汉堡包被用来捅人了吗？也不太可能。一个芝士汉堡刺伤了另一个芝士汉堡？不可能。  

The only plausible scenario was that someone had stabbed someone else over a cheeseburger. Computers, Choi said, are puzzled by this kind of problem. They lack the common sense to dismiss the possibility of food-on-food crime.  

唯一可能的情况是有人为了一个芝士汉堡刺伤了别人。崔说，计算机对这类问题感到困惑。他们缺乏常识，无法排除食品犯罪的可能性。

For certain kinds of tasks—playing chess, detecting tumors—artificial intelligence can rival or surpass human thinking. But the broader world presents endless unforeseen circumstances, and there A.I. often stumbles.  

对于某些任务--下棋、检测肿瘤--人工智能可以媲美甚至超越人类思维。但更广阔的世界呈现出无尽的不可预见的情况，还有人工智能。经常绊倒。  

Researchers speak of “corner cases,” which lie on the outskirts of the likely or anticipated; in such situations, human minds can rely on common sense to carry them through, but A.I. systems, which depend on prescribed rules or learned associations, often fail.  

研究人员称之为“角落病例”，即位于可能或预期之外的病例;在这种情况下，人类的大脑可以依靠常识来完成任务，但人工智能。依赖于规定规则或习得关联的系统常常失败。

By definition, common sense is something everyone has; it doesn’t sound like a big deal. But imagine living without it and it comes into clearer focus.  

根据定义，常识是每个人都拥有的东西;听起来没什么大不了。但是想象一下没有它的生活，它会变得更加清晰。  

Suppose you’re a robot visiting a carnival, and you confront a fun-house mirror; bereft of common sense, you might wonder if your body has suddenly changed.  

假设你是一个参观嘉年华的机器人，面对着游乐场的镜子;失去了常识，你可能会怀疑你的身体是否突然发生了变化。  

On the way home, you see that a fire hydrant has erupted, showering the road; you can’t determine if it’s safe to drive through the spray. You park outside a drugstore, and a man on the sidewalk screams for help, bleeding profusely.  

在回家的路上，你看到一个消防栓喷了出来，洒在路上;你无法确定穿过喷雾是否安全。你把车停在一家药店外面，人行道上的一个男人尖叫着求救，流着大量的血。  

Are you allowed to grab bandages from the store without waiting in line to pay? At home, there’s a news report—something about a cheeseburger stabbing. As a human being, you can draw on a vast reservoir of implicit knowledge to interpret these situations.  

你可以不排队付款就从商店里拿绷带吗？在家里，有一个新闻报道关于一个奶酪汉堡刺伤。作为一个人，你可以利用大量的隐性知识来解释这些情况。  

You do so all the time, because life is cornery. A.I.s are likely to get stuck.  

你总是这样，因为生活就是玉米地。人工智能很可能会陷入困境。

Oren Etzioni, the C.E.O. of the Allen Institute for Artificial Intelligence, in Seattle, told me that common sense is “the dark matter” of A.I. It “shapes so much of what we do and what we need to do, and yet it’s ineffable,” he added.  

奥伦·埃齐奥尼，首席执行官.西雅图艾伦人工智能研究所的一位教授告诉我，常识是人工智能的“暗物质”。它“塑造了我们所做的和我们需要做的很多事情，但它是无法形容的，”他补充说。  

The Allen Institute is working on the topic with the Defense Advanced Research Projects Agency (_D_AR_PA_), which launched a four-year, seventy-million-dollar effort called Machine Common Sense in 2019. If computer scientists could give their A.I. systems common sense, many thorny problems would be solved. As one [review article](https://dl.acm.org/doi/10.1145/2701413) noted, A.I. looking at a sliver of wood peeking above a table would know that it was probably part of a chair, rather than a random plank. A language-translation system could untangle ambiguities and double meanings.  

艾伦研究所正在与美国国防高级研究计划局（D_AR_PA）合作研究这一课题，D_AR_PA于2019年启动了一项为期四年、耗资7000万美元的“机器常识”项目。如果计算机科学家能给予他们的人工智能。系统常识，许多棘手的问题就迎刃而解了。正如一篇评论文章所指出的，A.I.从桌子上方偷看一小块木头，就知道它可能是椅子的一部分，而不是一块随意的木板。语言翻译系统可以解决歧义和双重含义。  

A house-cleaning robot would understand that a cat should be neither disposed of nor placed in a drawer. Such systems would be able to function in the world because they possess the kind of knowledge we take for granted.  

一个打扫房间的机器人会明白，猫既不应该被处理掉，也不应该放在抽屉里。这样的系统能够在世界上发挥作用，是因为它们拥有我们认为理所当然的知识。

\[_Support The New Yorker’s award-winning journalism. [Subscribe today »](https://subscribe.newyorker.com/subscribe/splits/newyorker/NYR_Generic?source=HCL_NYR_IN_CONTENT_SUBSCRIBE_0_ZZ)_\]  

\[支持《纽约客》屡获殊荣的新闻报道。立即订阅\]

In the nineteen-nineties, questions about [A.I. and safety](https://www.sciencenews.org/century/computer-ai-algorithm-moore-law-ethics#ethical-issues) helped drive Etzioni to begin studying common sense.  

在20世纪90年代，关于人工智能的问题和安全感促使Etzioni开始学习常识。  

In 1994, he co-authored a paper attempting to formalize the “first law of robotics”—a fictional rule in the sci-fi novels of Isaac Asimov that states that “a robot may not injure a human being or, through inaction, allow a human being to come to harm.  

1994年，他与人合著了一篇论文，试图将“机器人第一定律”正式化"-这是艾萨克·阿西莫夫科幻小说中虚构的一条规则，规定“机器人不得伤害人类，或因不作为而允许人类受到伤害。  

” The problem, he found, was that computers have no _notion_ of harm. That sort of understanding would require a broad and basic comprehension of a person’s needs, values, and priorities; without it, mistakes are nearly inevitable. In 2003, [the philosopher Nick Bostrom](https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom) imagined an A.I. program tasked with maximizing paper-clip production; it realizes that people might turn it off and so does away with them in order to complete its mission.  

“他发现，问题在于计算机没有伤害的概念。这种理解需要对一个人的需求、价值观和优先事项有广泛而基本的理解;没有它，错误几乎不可避免。2003年，哲学家尼克·博斯特罗姆（Nick Bostrom）设想了一种人工智能。计划的任务是最大限度地提高回形针生产;它意识到人们可能会把它关掉，所以为了完成它的使命，它会把他们赶走。

Bostrom’s paper-clip A.I. lacks moral common sense—it might tell itself that messy, unclipped documents are a form of harm. But perceptual common sense is also a challenge. In recent years, computer scientists have begun cataloguing examples of [“adversarial” inputs](https://www.science.org/content/article/turtle-or-rifle-hackers-easily-fool-ais-seeing-wrong-thing)—small changes to the world that confuse computers trying to navigate it. In one study, the strategic placement of a few small stickers on a stop sign made a computer vision system see it as a speed-limit sign.  

博斯特罗姆的回形针人工智能。缺乏道德常识--它可能会告诉自己，凌乱的、未经剪辑的文档是一种伤害。但感知常识也是一个挑战。近年来，计算机科学家开始对“对抗性”输入的例子进行分类--世界上的微小变化会让试图导航的计算机感到困惑。在一项研究中，在停车标志上巧妙地放置几个小贴纸，让计算机视觉系统将其视为限速标志。  

In another study, subtly changing the pattern on a 3-D-printed turtle made an A.I. computer program see it as a rifle. A.I. with common sense wouldn’t be so easily perplexed—it would know that rifles don’t have four legs and a shell.  

在另一项研究中，通过巧妙地改变3D打印海龟的图案，可以制造出一个人工智能。计算机程序把它看作是步枪。人工智能有常识的人就不会那么容易困惑了--他们知道步枪没有四条腿和一个弹壳。

Choi, who teaches at the University of Washington and works with the Allen Institute, told me that, in the nineteen-seventies and eighties, A.I. researchers thought that they were close to programming common sense into computers.  

崔教授在华盛顿大学任教，并在艾伦研究所工作。他告诉我，在20世纪70年代和80年代，人工智能。研究人员认为他们已经接近于将常识编入计算机。  

“But then they realized ‘Oh, that’s just too hard,’ ” she said; they turned to “easier” problems, such as object recognition and language translation, instead. Today the picture looks different. Many A.I.  

"但后来他们意识到"哦，那太难了，""她说;他们转而研究"更简单"的问题，如物体识别和语言翻译。今天，情况看起来有所不同。很多人工智能。  

systems, such as driverless cars, may soon be working regularly alongside us in the real world; this makes the need for artificial common sense more acute. And common sense may also be more attainable.  

无人驾驶汽车等系统可能很快就会在现实世界中与我们一起正常工作;这使得对人工常识的需求更加迫切。而且常识也可能更容易获得。  

Computers are getting better at learning for themselves, and researchers are learning to feed them the right kinds of data. A.I. may soon be covering more corners.  

计算机越来越善于自我学习，研究人员也在学习如何向它们提供正确的数据。人工智能可能很快就会覆盖更多的角落。

How do human beings acquire common sense? The short answer is that we’re multifaceted learners. We try things out and observe the results, read books and listen to instructions, absorb silently and reason on our own. We fall on our faces and watch others make mistakes. A.I.  

人类是怎样获得常识的？简短的回答是，我们是多方面的学习者。我们尝试事物并观察结果，阅读书籍并听取指示，默默吸收并自己推理。我们就掉在地上，看着别人犯错。人工智能  

systems, by contrast, aren’t as well-rounded. They tend to follow one route at the exclusion of all others.  

相比之下，系统就不那么全面了。他们倾向于走一条路而排斥所有其他的路。

Early researchers followed the explicit-instructions route. In 1984, a computer scientist named Doug Lenat began building Cyc, a kind of encyclopedia of common sense based on axioms, or rules, that explain how the world works.  

早期的研究人员遵循的是明确的指导路线。1984年，一位名叫道格·莱纳特的计算机科学家开始建立Cyc，这是一种基于公理或规则的常识百科全书，解释了世界是如何运作的。  

One axiom might hold that owning something means owning its parts; another might describe how hard things can damage soft things; a third might explain that flesh is softer than metal.  

一个公理可能认为，拥有某物意味着拥有它的各个部分;另一种可能是描述坚硬的东西如何能够破坏柔软的东西;第三种解释可能是肉体比金属软。  

Combine the axioms and you come to common-sense conclusions: if the bumper of your driverless car hits someone’s leg, you’re responsible for the hurt. “It’s basically representing and reasoning in real time with complicated nested-modal expressions,” Lenat told me.  

把这些公理结合起来，你就会得出常识性的结论：如果你的无人驾驶汽车的保险杠撞到了别人的腿，你要对受伤负责。“它基本上是用复杂的嵌套模态表达式真实的表示和推理，”Lenat告诉我。  

Cycorp, the company that owns Cyc, is still a going concern, and hundreds of logicians have spent decades inputting tens of millions of axioms into the system; the firm’s products are shrouded in secrecy, but Stephen DeAngelis, the C.E.O.  

拥有Cyc的Cycorp公司仍然是一家持续经营的公司，数百名逻辑学家花了几十年的时间向系统中输入了数千万条公理;公司的产品都是保密的，但首席执行官斯蒂芬·迪安杰利斯  

of Enterra Solutions, which advises manufacturing and retail companies, told me that its software can be powerful.  

为制造业和零售业提供咨询服务的Enterra Solutions公司的总裁告诉我，该公司的软件功能强大。  

He offered a culinary example: Cyc, he said, possesses enough common-sense knowledge about the “flavor profiles” of various fruits and vegetables to reason that, even though a tomato is a fruit, it shouldn’t go into a fruit salad.  

他举了一个烹饪的例子：他说，Cyc对各种水果和蔬菜的“风味特征”有足够的常识，可以推理出，即使西红柿是水果，也不应该放在水果沙拉里。

Academics tend to see Cyc’s approach as outmoded and labor-intensive; they doubt that the nuances of common sense can be captured through axioms.  

学术界倾向于认为Cyc的方法过时且劳动密集型;他们怀疑常识的细微差别能否通过公理来把握。  

Instead, they focus on machine learning, the technology behind Siri, Alexa, Google Translate, and other services, which works by detecting patterns in vast amounts of data. Instead of reading an instruction manual, machine-learning systems analyze the library.  

相反，他们专注于机器学习，这是Siri、Alexa、谷歌翻译和其他服务背后的技术，通过检测大量数据中的模式来工作。机器学习系统分析库，而不是阅读说明书。  

In 2020, the research lab _OpenAI_ revealed a machine-learning algorithm called [GPT-3](https://www.nature.com/articles/d41586-021-00530-0); it looked at text from the World Wide Web and discovered linguistic patterns that allowed it to produce plausibly human writing from scratch. GPT-3’s mimicry is stunning in some ways, but it’s underwhelming in others.  

2020年，研究实验室_OpenAI_揭示了一种名为GPT-3的机器学习算法;它研究了万维网上的文本，发现了语言模式，使它能够从头开始产生看似真实的人类文字。GPT-3的模仿在某些方面令人惊叹，但在其他方面却平淡无奇。  

The system can still produce strange statements: for example, “It takes two rainbows to jump from Hawaii to seventeen.” If GPT-3 had common sense, it would know that rainbows aren’t units of time and that seventeen is not a place.  

系统仍然可以产生奇怪的语句：例如，“从夏威夷跳到十七，需要两道彩虹。”如果GPT-3有常识的话，它就会知道彩虹不是时间单位，17不是一个地方。

Choi’s team is trying to use language models like GPT-3 as stepping stones to common sense.  

Choi的团队正试图使用GPT-3这样的语言模型作为通往常识的垫脚石。  

In one line of research, they asked GPT-3 to generate millions of plausible, common-sense statements describing causes, effects, and intentions—for example, “Before Lindsay gets a job offer, Lindsay has to apply.  

在一项研究中，他们要求GPT-3生成数百万个描述原因、效果和意图的看似合理的常识性陈述--例如，“在林赛得到工作机会之前，林赛必须申请。  

” They then asked a second machine-learning system to analyze a filtered set of those statements, with an eye to completing fill-in-the-blank questions. (“Alex makes Chris wait. Alex is seen as . . .  

“然后，他们要求第二个机器学习系统分析这些语句的过滤集，着眼于完成填空题。（“亚历克斯让克里斯等着。亚历克斯被视为。。。  

”) Human evaluators found that the completed sentences produced by the system were commonsensical eighty-eight per cent of the time—a marked improvement over GPT-3, which was only seventy-three-per-cent commonsensical.  

“）人工评估人员发现，该系统生成的完整句子在88%的时间里符合常识，这比GPT-3有了显著改进，后者只有73%符合常识。

Choi’s lab has done something similar with short videos. She and her collaborators first created a database of millions of captioned clips, then asked a machine-learning system to analyze them.  

Choi的实验室在短视频方面也做了类似的事情。她和她的合作者首先创建了一个包含数百万个字幕剪辑的数据库，然后让一个机器学习系统对它们进行分析。  

Meanwhile, online crowdworkers—Internet users who perform tasks for pay—composed multiple-choice questions about still frames taken from a second set of clips, which the A.I. had never seen, and multiple-choice questions asking for justifications to the answer.  

与此同时，在线众包工作者--为付费而执行任务的互联网用户--针对第二组视频片段中的静止帧设计了多项选择题。从来没有见过，多项选择题要求理由的答案。  

A typical frame, taken from the movie “Swingers,” shows a waitress delivering pancakes to three men in a diner, with one of the men pointing at another. In response to the question “Why is \[person4\] pointing at \[person1\]?  

一个典型的画面，取自电影"摇摆人"，显示一名女服务员在一家餐厅给三名男子送煎饼，其中一名男子指着另一名男子。对于问题"\[person4\]为什么指向\[person1\]？  

,” the system said that the pointing man was “telling \[person3\] that \[person1\] ordered the pancakes.” Asked to explain its answer, the program said that “\[person3\] is delivering food to the table, and she might not know whose order is whose.” The A.I.  

"，系统说指点者是在"告诉\[人3\]\[人1\]点了煎饼。"当被要求解释其答案时，该程序表示"（person3）正在将食物送上餐桌，她可能不知道谁的订单是谁的。"人工智能。  

answered the questions in a commonsense way seventy-two per cent of the time, compared with eighty-six per cent for humans.  

在72%的时间里，他们以常识的方式回答了这些问题，而人类只有86%。  

Such systems are impressive—they seem to have enough common sense to understand everyday situations in terms of physics, cause and effect, and even psychology.  

这样的系统令人印象深刻它们似乎有足够的常识来理解日常生活中的物理现象、因果关系，甚至心理学。  

It’s as though they know that people eat pancakes in diners, that each diner has a different order, and that pointing is a way of delivering information.  

这就好像他们知道人们在餐车里吃煎饼，每个餐车都有不同的订单，而指点是一种传递信息的方式。
